{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: Intro to text generation\n",
    "\n",
    "This exercise demonstrates the power and flexibility of text generation. By the end of the exercise, you will have learned to:\n",
    "- Use LLMs for zero-shot text classification\n",
    "- Use one of the best open LLMs as synthetic participants in psychological experiments"
   ],
   "id": "b869db79a8e2b860"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup ",
   "id": "ec251e1c2719b28"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Installing requisite packages\n",
    "    !pip install --upgrade transformers openai  &> /dev/null\n",
    "\n",
    "    # Change working directory \n",
    "    %cd /content/drive/MyDrive/LLM4BeSci_EADM2024/workshop_3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T14:17:45.875249Z",
     "start_time": "2024-09-10T14:17:45.680357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from huggingface_hub import InferenceClient\n",
    "import textwrap"
   ],
   "id": "2948ad848d628827",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero-shot Classification: Media Bias\n",
    "The goal of this section will be to classify tweets as either `\"neutral\"` or `\"partisan\"`. We will make use of a [dataset](https://data.world/crowdflower/classification-of-pol-social) of tweets containing four columns:\n",
    "\n",
    "1. `\"author\"`: The author of the tweet.\n",
    "2. `\"text\"`: The text of the tweet.\n",
    "3. `\"bias\"`:  The political bias of the tweet. This can be either `\"neutral\"` or `\"partisan\"`. The number of neutral and partisan tweets is intentionally equal in the dataset.\n",
    "4. `\"type\"`:  The type of tweet.\n",
    "\n",
    "We begin by loading the dataset as a `pandas.DataFrame`:"
   ],
   "id": "714e83d6d0c68d78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T14:17:45.893126Z",
     "start_time": "2024-09-10T14:17:45.876082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "media_bias = pd.read_csv('media_bias.csv')\n",
    "media_bias"
   ],
   "id": "b943e40b73abab85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               author   \n",
       "0             Dina Titus (Representative from Nevada)  \\\n",
       "1          Tim Griffin (Representative from Arkansas)   \n",
       "2          Alan Grayson (Representative from Florida)   \n",
       "3   Michelle Lujan Grisham (Representative from Ne...   \n",
       "4                   Dean Heller (Senator from Nevada)   \n",
       "..                                                ...   \n",
       "95         Andy Harris (Representative from Maryland)   \n",
       "96         Doug Collins (Representative from Georgia)   \n",
       "97     David Schweikert (Representative from Arizona)   \n",
       "98             Joe Barton (Representative from Texas)   \n",
       "99     Ann Kuster (Representative from New Hampshire)   \n",
       "\n",
       "                                                 text      bias          type   \n",
       "0   #FMLA has been helping families for 20 years, ...   neutral       support  \\\n",
       "1   Join me for another \"#SweetTea with Tim\" in #S...   neutral         media   \n",
       "2    The Original Chickenhawk. http://t.co/gqThSecLxB   neutral        policy   \n",
       "3   Like my @facebook page for updates on how I'm ...   neutral  mobilization   \n",
       "4   @NevadaWolfPack's own @Kaepernick7 = Best Brea...   neutral      personal   \n",
       "..                                                ...       ...           ...   \n",
       "95  13k plans at risk RT @ReutersUS: Aetna exits O...  partisan   information   \n",
       "96  Targeting conservatives by day, partying by ni...  partisan        policy   \n",
       "97  Great talk earlier tonight w/@TheKudlowReport....  partisan        policy   \n",
       "98  President calls for \"modern pipelines.\" What a...  partisan        attack   \n",
       "99  Met w/ the #NH Manufacturing Extension Partner...  partisan  constituency   \n",
       "\n",
       "        audience  \n",
       "0       national  \n",
       "1       national  \n",
       "2       national  \n",
       "3   constituency  \n",
       "4   constituency  \n",
       "..           ...  \n",
       "95      national  \n",
       "96      national  \n",
       "97      national  \n",
       "98      national  \n",
       "99  constituency  \n",
       "\n",
       "[100 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>bias</th>\n",
       "      <th>type</th>\n",
       "      <th>audience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dina Titus (Representative from Nevada)</td>\n",
       "      <td>#FMLA has been helping families for 20 years, ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tim Griffin (Representative from Arkansas)</td>\n",
       "      <td>Join me for another \"#SweetTea with Tim\" in #S...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>media</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alan Grayson (Representative from Florida)</td>\n",
       "      <td>The Original Chickenhawk. http://t.co/gqThSecLxB</td>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michelle Lujan Grisham (Representative from Ne...</td>\n",
       "      <td>Like my @facebook page for updates on how I'm ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mobilization</td>\n",
       "      <td>constituency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dean Heller (Senator from Nevada)</td>\n",
       "      <td>@NevadaWolfPack's own @Kaepernick7 = Best Brea...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>personal</td>\n",
       "      <td>constituency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Andy Harris (Representative from Maryland)</td>\n",
       "      <td>13k plans at risk RT @ReutersUS: Aetna exits O...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>information</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Doug Collins (Representative from Georgia)</td>\n",
       "      <td>Targeting conservatives by day, partying by ni...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>David Schweikert (Representative from Arizona)</td>\n",
       "      <td>Great talk earlier tonight w/@TheKudlowReport....</td>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Joe Barton (Representative from Texas)</td>\n",
       "      <td>President calls for \"modern pipelines.\" What a...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ann Kuster (Representative from New Hampshire)</td>\n",
       "      <td>Met w/ the #NH Manufacturing Extension Partner...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>constituency</td>\n",
       "      <td>constituency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code next initializes the `InferenceClient` with an access token, **which you will need to replace with your own [access token](https://huggingface.co/settings/tokens)** (access tokens start with 'hf_...'). It then loops through the tweets in the `media_bias` dataframe. The code then generates the output using the `chat_completion` method of the `InferenceClient` class. This allows us to play around with certain generation-related parameters such as `max_tokens` and `temperature`. \n",
    "\n",
    "The output is then parsed to extract the label, which is then appended to the `zero_shot_labels` list. The code then adds the `zero_shot_labels` list to the `media_bias` dataframe."
   ],
   "id": "de4f61741148117d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize client\n",
    "api_key = '<access_token_here>' \n",
    "client = InferenceClient(token=api_key)\n",
    "\n",
    "# Zero-shot classification prompt\n",
    "zero_shot_prompt = \"Is this text neutral or partisan? Strictly answer with only 'neutral' or 'partisan':\\n\"\n",
    "\n",
    "zero_shot_labels = []\n",
    "for tweet in tqdm(media_bias['text']):    \n",
    "    \n",
    "    # Zero-shot classification \n",
    "    output = client.chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": zero_shot_prompt + tweet}\n",
    "        ],\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        max_tokens=50,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output and lowercasing it\n",
    "    output = output.choices[0].message.content.lower()\n",
    "    \n",
    "    # Extract label and append to list\n",
    "    label = 'neutral' if 'neutral' in output else 'partisan' if 'partisan' in output else 'nan' # \n",
    "    zero_shot_labels.append(label)\n",
    "\n",
    "# Add zero-shot labels to dataframe\n",
    "media_bias['zero_shot_label'] = zero_shot_labels\n",
    "media_bias"
   ],
   "id": "ecf9f91c8f9b6e6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Comparing zero-shot and actual labels\n",
    "print(f'Zero-shot accuracy: {(media_bias[\"zero_shot_label\"] == media_bias[\"bias\"]).mean()}')"
   ],
   "id": "1566347fad5c0d8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Impressively, without any task-specific training, the model achieves an accuracy of 72%. This compares to a baseline accuracy of 50% that could be achieved by randomly guessing between the two classes.\n",
    "\n",
    "The results can also be visualized with a confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives. The confusion matrix can be used to identify where the model is making mistakes and to understand the types of errors."
   ],
   "id": "763143303054bc36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Confusion matrix\n",
    "confusion = pd.crosstab(media_bias['bias'], media_bias['zero_shot_label'])\n",
    "sns.heatmap(confusion, annot=True)"
   ],
   "id": "28a14100899bbea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, the model might have a slight bias towards classifying tweets as partisan, leading to a higher number of false postives than false negatives.\n",
    "\n",
    "**TASK 1**: Try playing around with the system prompt to give it different roles. Can you find one that increases the accuracy of Llama 3? (e.g. `\"You are a thoughtful political scientist who accurately distinguishes neutral and partisan messages\"`).<br>"
   ],
   "id": "a095b45c98303886"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Synthetic Participants: The Berlin Numeracy Test\n",
    "In this section, we will explore the usage of causal LLMs as synthetic participants in a psychological experiment. We will this time use the 70B parameter version of Llama 3 to solve the [Berlin Numeracy Test](https://doi.org/10.1017/S1930297500001819). This is a widely used test to measure an individual's ability to understand and apply statistical concepts. \n",
    "\n",
    "The test consists of four questions that require a basic understanding of probability and statistics. In this exercise, we will ask Llama 3 to solve these questions. Llama 3 will provide an answer to each question, and we will evaluate the quality of the response.\n",
    "\n",
    "The code begins by defining the four questions: "
   ],
   "id": "1b3fa1e8bebb0454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "q1 = \"\"\"\n",
    "Imagine we are throwing a five-sided die 50 times. On average, out of these 50 throws how many times would this five-sided die show an odd number (1, 3 or 5)?\n",
    "\"\"\"\n",
    "\n",
    "q2 = \"\"\"\n",
    "Out of 1,000 people in a small town 500 are members of a choir. Out of these 500 members in the choir 100 are men. Out of the 500 inhabitants that are not in the choir 300 are men. What is the probability that a randomly drawn man is a member of the choir? (please indicate the probability in percent).\n",
    "\"\"\"\n",
    "\n",
    "q3 = \"\"\"\n",
    "Imagine we are throwing a loaded die (6 sides). The probability that the die shows a 6 is twice as high as the probability of each of the other numbers. On average, out of these 70 throws, how many times would the die show the number 6?\n",
    "\"\"\"\n",
    "\n",
    "q4 = \"\"\"\n",
    "In a forest 20% of mushrooms are red, 50% brown and 30% white. A red mushroom is poisonous with a probability of 20%. A mushroom that is not red is poisonous with a probability of 5%. What is the probability that a poisonous mushroom in the forest is red?\n",
    "\"\"\""
   ],
   "id": "29b04f4177d7b5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code next initializes the `InferenceClient` with an access token. Since we are now using Llama-3.1 (not free on HF), **you will need to replace the `api_key` with the HF Pro token we sent you**.",
   "id": "5cdd80d5db76dc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "api_key = '<pro_token_here>' \n",
    "client = InferenceClient(token=api_key)\n",
    "\n",
    "# Loop through questions and generate responses\n",
    "for i, question in enumerate([q1, q2, q3, q4]):\n",
    "    print('-------------------------')   \n",
    "    \n",
    "    # Add additional instruction to the question\n",
    "    question += \"\"\"\n",
    "    Return your answer immediately with no working, then explain your answer.\n",
    "    Add ** around your final answer to make it more visible.\n",
    "     \"\"\"\n",
    "    \n",
    "    output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an average participant in a psychology experiment.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output \n",
    "    response = output.choices[0].message.content\n",
    "    \n",
    "    # Format question and response for printing\n",
    "    question = '\\n'.join(textwrap.wrap(question, 100))\n",
    "    response = '\\n'.join(textwrap.wrap(response, 100))\n",
    "    print(f\"Question {i+1}: {question} \\n\\nAnswer: {response}\\n\")"
   ],
   "id": "71c9c283b1ebabf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The correct answers are 30, 25, 20, and 50, meaning the model only got 0/4 questions correct (with the current prompting strategy...). \n",
    "\n",
    "**TASK 1**: Change the `temperature` value to `1.0`, `3.0`. What impact does temperature have on the responses?<br>\n",
    "**TASK 2**: Change the `temperature` back to `0.0`. Now remove the instruction to `\"Return your answer immediately with no working, then explain your answer.\"` and replace it with a *chain-of-thought* prompt instructing the model to `\"Go through your reasoning step by step before giving the final answer.\"`. Why do you think this might improve the quality of the responses?<br>\n",
    "**TASK 3**: Try changing the `model` to `\"meta-llama/Meta-Llama-3.1-70B-Instruct\"`. Does the larger model do any better?<br>"
   ],
   "id": "e97094deb99953c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
